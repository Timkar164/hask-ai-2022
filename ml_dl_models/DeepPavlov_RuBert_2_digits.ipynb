{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e25d7265",
   "metadata": {},
   "source": [
    "# Классификация текста с помощью трансформера BERT\n",
    "\n",
    "Оригинальная идея подчерпнута отсюда https://www.kaggle.com/c/learn-ai-bbc и отсюда https://habr.com/ru/post/655517/\n",
    "\n",
    "\n",
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed286f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f03ae46",
   "metadata": {},
   "source": [
    "### Чтение нашего датасета, состоящего описаний категорий и ТНВЭД\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "376589f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OPISANIE_CLEAN</th>\n",
       "      <th>TNVED</th>\n",
       "      <th>TNVED2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>части принадлежности аппаратуры поз 9025 военн...</td>\n",
       "      <td>9025</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>трансформаторы мощностью 1 ква 16 ква лом элек...</td>\n",
       "      <td>8504</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>клапаны запорные стали военного назначения</td>\n",
       "      <td>8481</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>устройства сигнализационные охранные устройств...</td>\n",
       "      <td>8531</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>вещества поверхностно активные органические ан...</td>\n",
       "      <td>3402</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177894</th>\n",
       "      <td>минеральное моторное масло всесезонное дизельн...</td>\n",
       "      <td>2710</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177895</th>\n",
       "      <td>синтетическое моторное масло всесезонное дизел...</td>\n",
       "      <td>3403</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177896</th>\n",
       "      <td>синтетическое моторное масло всесезонное дизел...</td>\n",
       "      <td>3403</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177897</th>\n",
       "      <td>масло трансмиссионное синтетической основе сод...</td>\n",
       "      <td>3403</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177898</th>\n",
       "      <td>масло трансмиссионное синтетической основе сод...</td>\n",
       "      <td>3403</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177899 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            OPISANIE_CLEAN  TNVED  TNVED2\n",
       "0        части принадлежности аппаратуры поз 9025 военн...   9025      90\n",
       "1        трансформаторы мощностью 1 ква 16 ква лом элек...   8504      85\n",
       "2               клапаны запорные стали военного назначения   8481      84\n",
       "3        устройства сигнализационные охранные устройств...   8531      85\n",
       "4        вещества поверхностно активные органические ан...   3402      34\n",
       "...                                                    ...    ...     ...\n",
       "4177894  минеральное моторное масло всесезонное дизельн...   2710      27\n",
       "4177895  синтетическое моторное масло всесезонное дизел...   3403      34\n",
       "4177896  синтетическое моторное масло всесезонное дизел...   3403      34\n",
       "4177897  масло трансмиссионное синтетической основе сод...   3403      34\n",
       "4177898  масло трансмиссионное синтетической основе сод...   3403      34\n",
       "\n",
       "[4177899 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv', index_col=0)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2404f2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OPISANIE_CLEAN</th>\n",
       "      <th>TNVED</th>\n",
       "      <th>TNVED2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>лошади живые чистопородные племенные животные</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>лошади ослы мулы лошаки живые прочие</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>прочие лошади живые прочие</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ослы живые</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>прочие мулы лошаки живые</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12318</th>\n",
       "      <td>подлинники гравюр эстампов литографий</td>\n",
       "      <td>9702</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12319</th>\n",
       "      <td>подлинники скульптур статуэток любых материалов</td>\n",
       "      <td>9703</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12320</th>\n",
       "      <td>марки почтовые марки госпошлин знаки почтовой ...</td>\n",
       "      <td>9704</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12321</th>\n",
       "      <td>коллекции предметы коллекционирования зоологии...</td>\n",
       "      <td>9705</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12322</th>\n",
       "      <td>антиквариат возрастом 100 лет</td>\n",
       "      <td>9706</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12322 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          OPISANIE_CLEAN  TNVED  TNVED2\n",
       "                                                                       \n",
       "1          лошади живые чистопородные племенные животные    101       1\n",
       "2                   лошади ослы мулы лошаки живые прочие    101       1\n",
       "3                             прочие лошади живые прочие    101       1\n",
       "4                                             ослы живые    101       1\n",
       "5                               прочие мулы лошаки живые    101       1\n",
       "...                                                  ...    ...     ...\n",
       "12318              подлинники гравюр эстампов литографий   9702      97\n",
       "12319    подлинники скульптур статуэток любых материалов   9703      97\n",
       "12320  марки почтовые марки госпошлин знаки почтовой ...   9704      97\n",
       "12321  коллекции предметы коллекционирования зоологии...   9705      97\n",
       "12322                      антиквариат возрастом 100 лет   9706      97\n",
       "\n",
       "[12322 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2 = pd.read_csv('tnved.csv', index_col=0)\n",
    "df2 = df2.rename(columns={'TNVED_SHORT': 'TNVED'})\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5802dcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OPISANIE_CLEAN</th>\n",
       "      <th>TNVED</th>\n",
       "      <th>TNVED2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3765276</th>\n",
       "      <td>крепежные изделия фурнитура пластмасс ввозятся...</td>\n",
       "      <td>3926</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4039130</th>\n",
       "      <td>карты флэш памяти usb накопители содержащие за...</td>\n",
       "      <td>8523</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549633</th>\n",
       "      <td>шурупы дерева снабженные резьбой пазов насечек...</td>\n",
       "      <td>7318</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484730</th>\n",
       "      <td>виноград свежий столовый сорт мерседес 16 июля...</td>\n",
       "      <td>806</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990828</th>\n",
       "      <td>автомобиль марки volkswagen модели polo vin xw...</td>\n",
       "      <td>8703</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12318</th>\n",
       "      <td>подлинники гравюр эстампов литографий</td>\n",
       "      <td>9702</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12319</th>\n",
       "      <td>подлинники скульптур статуэток любых материалов</td>\n",
       "      <td>9703</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12320</th>\n",
       "      <td>марки почтовые марки госпошлин знаки почтовой ...</td>\n",
       "      <td>9704</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12321</th>\n",
       "      <td>коллекции предметы коллекционирования зоологии...</td>\n",
       "      <td>9705</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12322</th>\n",
       "      <td>антиквариат возрастом 100 лет</td>\n",
       "      <td>9706</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112322 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            OPISANIE_CLEAN  TNVED  TNVED2\n",
       "3765276  крепежные изделия фурнитура пластмасс ввозятся...   3926      38\n",
       "4039130  карты флэш памяти usb накопители содержащие за...   8523      83\n",
       "1549633  шурупы дерева снабженные резьбой пазов насечек...   7318      72\n",
       "484730   виноград свежий столовый сорт мерседес 16 июля...    806       7\n",
       "3990828  автомобиль марки volkswagen модели polo vin xw...   8703      85\n",
       "...                                                    ...    ...     ...\n",
       "12318                подлинники гравюр эстампов литографий   9702      95\n",
       "12319      подлинники скульптур статуэток любых материалов   9703      95\n",
       "12320    марки почтовые марки госпошлин знаки почтовой ...   9704      95\n",
       "12321    коллекции предметы коллекционирования зоологии...   9705      95\n",
       "12322                        антиквариат возрастом 100 лет   9706      95\n",
       "\n",
       "[112322 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = df.sample(n = 100000)\n",
    "df_train = pd.concat([df_train,df2])\n",
    "\n",
    "df_train['TNVED2']=df_train['TNVED2'].apply(lambda x: x - 2 if x > 77 else x-1)\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17c9285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18ba8ec0",
   "metadata": {},
   "source": [
    "### Загрузка претренированной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb5be876",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = AutoModel.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0097395c",
   "metadata": {},
   "source": [
    "### Разбиение выборок на текст и таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97028658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('X_train.pickle', 'rb') as handle:\n",
    "    X_train = pickle.load(handle)\n",
    "with open('y_train.pickle', 'rb') as handle:\n",
    "    y_train = pickle.load(handle)\n",
    "with open('X_test.pickle', 'rb') as handle:\n",
    "    X_test = pickle.load(handle)\n",
    "with open('y_test.pickle', 'rb') as handle:\n",
    "    y_test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a61087cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train['OPISANIE_CLEAN'].astype(str)\n",
    "y = df_train['TNVED2'].astype(int)\n",
    "\n",
    "X_train, y_train = X,y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76639861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('X_train.pickle', 'wb') as handle:\n",
    "    pickle.dump(X_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('y_train.pickle', 'wb') as handle:\n",
    "    pickle.dump(y_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('X_test.pickle', 'wb') as handle:\n",
    "    pickle.dump(X_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('y_test.pickle', 'wb') as handle:\n",
    "    pickle.dump(y_test, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56bc3f",
   "metadata": {},
   "source": [
    "График длин текстов. Он поможет определить оптимальную длину последовательности токенов, чтобы избежать разреженных векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4734c6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(str(i).split()) for i in X_train]\n",
    "pd.Series(seq_len).hist(bins = 40, range =(0,40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d4be5",
   "metadata": {},
   "source": [
    "### Токенизация текста\n",
    "Берем длину 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af37964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokens_train.pickle', 'rb') as handle:\n",
    "    tokens_train = pickle.load(handle)\n",
    "    \n",
    "with open('tokens_test.pickle', 'rb') as handle:\n",
    "    tokens_test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3d8535",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_len = 15\n",
    "\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    X_train.values,\n",
    "    max_length = token_len,\n",
    "    padding = 'max_length',\n",
    "    truncation = True\n",
    ")\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    X_test.values,\n",
    "    max_length = token_len,\n",
    "    padding = 'max_length',\n",
    "    truncation = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba6732a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokens_train.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokens_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('tokens_test.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokens_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e379319f",
   "metadata": {},
   "source": [
    "### Создание датасета для обучения из токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7e72685",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(y_train.values)\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(y_test.values)\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
    "\n",
    "test_data =  TensorDataset(test_seq, test_mask, test_y)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3afaea26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([   101,  75642,  42359, 114243,  98937,   2237,  55337,    852,  34832,\n",
      "         27446,  28955,  72471,  29602,   8237,    102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(61))\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac29919",
   "metadata": {},
   "source": [
    "### Вместо обучения всего BERT добавим слой для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c2ae8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "class BERT_Arch(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        self.fc2 = nn.Linear(512,96)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    def forward(self, sent_id, mask):\n",
    "        _, cls_hs = self.bert(sent_id, attention_mask = mask, return_dict = False)\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2097e3",
   "metadata": {},
   "source": [
    "### Загрузка модели в GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc075547",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8d2d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_Arch(bert)\n",
    "\n",
    "model = model.to(device)\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a9b21",
   "metadata": {},
   "source": [
    "### Нормализация весов в зависимости от соотношения классов в выборке трейна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "999b321f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.69959192   2.03307292   1.62312675   2.93892104  24.68185307\n",
      "   1.34283207   1.68924766   0.99166038   3.88610827   7.42016076\n",
      "   7.25008013   6.01184979  34.71705247 104.05115741   4.32402017\n",
      "   2.42566096   3.11888661   2.72431548   2.1724726    1.43258555\n",
      "   2.09369087   0.78297605   4.87479596  12.69878941   4.48606833\n",
      "  14.9073082    0.87759542   1.81605739   1.06190315   1.07408142\n",
      "  11.32723394   1.65001781   0.90091856   1.22737159   5.7227904\n",
      "  49.31370614  12.8720605    1.4366821    0.25334791   0.2786298\n",
      "  13.05014468   1.27675022   8.33327802   0.38803193  46.85052083\n",
      "  21.32296402  12.20597944   0.98788619   3.34581133  39.05043403\n",
      "  15.91458333   4.77732534  28.41395202   5.9000651    5.25005787\n",
      "   4.9505781    7.14098801   9.6011267    5.55594363   8.26061769\n",
      "   0.25419075   0.28871727   1.7066556    0.62178401   3.77912517\n",
      "  34.71705247  18.40314542   1.69500952   1.87103194   1.00026438\n",
      "   6.46103025   1.08198502   0.20463579   2.40770886  21.81768411\n",
      "   1.54283958  55.10943627  36.05040064  85.14185606   7.5984711\n",
      "   0.74851524   0.78011733   0.13188351   0.13217826   9.05010016\n",
      "   0.21756363  20.85023148  18.40314542   0.44000434   2.71669634\n",
      "  11.75013021  37.49041667   0.64278684   1.43054634   1.42043985\n",
      "  66.9078869 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugene/miniconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass classes=[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95], y=3138558    60\n",
      "429607     92\n",
      "2614966    72\n",
      "2774424    18\n",
      "2940022    72\n",
      "           ..\n",
      "8234       72\n",
      "744973     83\n",
      "169952     47\n",
      "1512868    26\n",
      "3087034    83\n",
      "Name: TNVED2, Length: 89857, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weights += 0.05\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "weights = torch.tensor(class_weights, dtype = torch.float)\n",
    "weights = weights.to(device)\n",
    "#cross_entropy = nn.CrossEntropyLoss()\n",
    "cross_entropy = nn.NLLLoss(weight=weights)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c99b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab5fe6d2",
   "metadata": {},
   "source": [
    "Функция для наглядного обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04601ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    total_preds = []\n",
    "    \n",
    "    for step, batch in tqdm(enumerate(train_dataloader), total = len(train_dataloader)):\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id,mask,labels = batch\n",
    "        model.zero_grad()\n",
    "        preds = model(sent_id, mask)\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        #print(loss)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        total_preds.append(preds)\n",
    "        \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    total_preds = np.concatenate(total_preds, axis = 0)\n",
    "    \n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ac1e8",
   "metadata": {},
   "source": [
    "Функция для тестирования модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec91cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0,0\n",
    "    total_preds = []\n",
    "\n",
    "    for step, batch in tqdm(enumerate(test_dataloader), total = len(test_dataloader)):\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "            #labels= labels.unsqueeze(1)\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    avg_loss = total_loss / len(test_dataloader)\n",
    "    total_preds = np.concatenate(total_preds, axis = 0)\n",
    "    \n",
    "    \n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d85ca",
   "metadata": {},
   "source": [
    "Обучение новых слоев для задачи классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fcf8d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch1 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22465/22465 [06:40<00:00, 56.07it/s]\n",
      "100%|██████████| 5617/5617 [01:27<00:00, 63.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 2.534\n",
      "Test loss: 1.802\n",
      "\n",
      " Epoch2 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22465/22465 [08:43<00:00, 42.91it/s]\n",
      "100%|██████████| 5617/5617 [01:31<00:00, 61.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 2.155\n",
      "Test loss: 1.675\n",
      "\n",
      " Epoch3 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22465/22465 [08:41<00:00, 43.07it/s]\n",
      "100%|██████████| 5617/5617 [01:32<00:00, 60.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 2.071\n",
      "Test loss: 1.716\n",
      "\n",
      " Epoch4 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22465/22465 [08:41<00:00, 43.05it/s]\n",
      "100%|██████████| 5617/5617 [01:32<00:00, 60.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 2.058\n",
      "Test loss: 1.660\n",
      "\n",
      " Epoch5 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22465/22465 [08:41<00:00, 43.10it/s]\n",
      "100%|██████████| 5617/5617 [01:32<00:00, 60.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 2.048\n",
      "Test loss: 1.589\n",
      "\n",
      " Epoch6 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22465/22465 [08:42<00:00, 43.00it/s]\n",
      "100%|██████████| 5617/5617 [01:32<00:00, 60.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 2.039\n",
      "Test loss: 1.701\n",
      "\n",
      " Epoch7 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22465/22465 [08:41<00:00, 43.05it/s]\n",
      "100%|██████████| 5617/5617 [01:32<00:00, 60.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 2.019\n",
      "Test loss: 1.677\n",
      "\n",
      " Epoch8 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22465/22465 [08:42<00:00, 43.02it/s]\n",
      "100%|██████████| 5617/5617 [01:18<00:00, 71.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 2.019\n",
      "Test loss: 1.570\n",
      "\n",
      " Epoch9 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22465/22465 [08:42<00:00, 43.02it/s]\n",
      "100%|██████████| 5617/5617 [01:32<00:00, 60.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 2.007\n",
      "Test loss: 1.527\n",
      "\n",
      " Epoch10 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22465/22465 [08:42<00:00, 42.98it/s]\n",
      "100%|██████████| 5617/5617 [01:32<00:00, 60.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 2.001\n",
      "Test loss: 1.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22465/22465 [06:16<00:00, 59.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5617/5617 [01:33<00:00, 60.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_test_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('\\n Epoch{:} / {:}'.format(epoch+1, epochs))\n",
    "    \n",
    "    train_loss, _ = train()\n",
    "    test_loss, _ = evaluate()\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    print(f'\\nTraining loss: {train_loss:.3f}')\n",
    "    print(f'Test loss: {test_loss:.3f}')\n",
    "    \n",
    "    \n",
    "\n",
    "def train_acc():\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0,0\n",
    "    total_preds = []\n",
    "    total_acc_test = 0\n",
    "\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), total = len(train_dataloader)):\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "            #labels= labels.unsqueeze(1)\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "            \n",
    "            test_labels = labels.to(device)\n",
    "            acc = (preds.argmax(dim=1) == test_labels).sum().item()\n",
    "            total_acc_test += acc            \n",
    "            \n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "            \n",
    "\n",
    "\n",
    "    avg_loss = total_loss / len(test_dataloader)\n",
    "    total_preds = np.concatenate(total_preds, axis = 0)\n",
    "    \n",
    "    print(f'Train Accuracy: {total_acc_test / len(total_preds): .3f}')\n",
    "    return avg_loss, total_preds\n",
    "\n",
    "ta,tb = train_acc()    \n",
    "    \n",
    "    \n",
    "def acc():\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0,0\n",
    "    total_preds = []\n",
    "    total_acc_test = 0\n",
    "\n",
    "    for step, batch in tqdm(enumerate(test_dataloader), total = len(test_dataloader)):\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "            #labels= labels.unsqueeze(1)\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "            \n",
    "            test_labels = labels.to(device)\n",
    "            acc = (preds.argmax(dim=1) == test_labels).sum().item()\n",
    "            total_acc_test += acc            \n",
    "            \n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "            \n",
    "\n",
    "\n",
    "    avg_loss = total_loss / len(test_dataloader)\n",
    "    total_preds = np.concatenate(total_preds, axis = 0)\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(total_preds): .3f}')\n",
    "    return avg_loss, total_preds\n",
    "\n",
    "a,b = acc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fba2c4",
   "metadata": {},
   "source": [
    "Загружаем лучшую модель для предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd881941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT_Arch(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=96, bias=True)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9bf556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '96_v1_saved_weights.pt'\n",
    "#model.load_state_dict(torch.load(path))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f47c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00a2ab98",
   "metadata": {},
   "source": [
    "### Предсказываем\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f828f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dataloader(predict_dataloader):\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    \n",
    "    for step, batch in tqdm(enumerate(predict_dataloader), total = len(predict_dataloader)):\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "            preds = preds.detach().cpu().numpy()    \n",
    "            preds = preds.argmax(axis=1)\n",
    "            labels = labels.cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "            total_labels.append(labels)\n",
    "            \n",
    "    return total_preds, total_labels\n",
    "\n",
    "\n",
    "def predict_df(df):\n",
    "    df = df.copy()\n",
    "    batch_size = 1\n",
    "    tokens_predict = tokenizer.batch_encode_plus(\n",
    "        df['OPISANIE_CLEAN'].values,\n",
    "        max_length = 15,\n",
    "        padding = 'max_length',\n",
    "        truncation = True\n",
    "    )\n",
    "    \n",
    "    total_preds = []\n",
    "    \n",
    "    predict_seq = torch.tensor(tokens_predict['input_ids'])\n",
    "    predict_mask = torch.tensor(tokens_predict['attention_mask'])\n",
    "    predict_data =  TensorDataset(predict_seq, predict_mask)\n",
    "    predict_sampler = SequentialSampler(predict_data)\n",
    "    predict_dataloader = DataLoader(predict_data, sampler = predict_sampler, batch_size = batch_size)\n",
    "    \n",
    "    \n",
    "    for step, batch in tqdm(enumerate(predict_dataloader), total = len(predict_dataloader)):\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "            preds = preds.detach().cpu().numpy()    \n",
    "            preds = np.argmax(preds)\n",
    "            total_preds.append(preds)\n",
    "    df['target'] = total_preds\n",
    "    return df\n",
    "\n",
    "#pred_df = predict_df(test_df)\n",
    "#display(pred_df)\n",
    "\n",
    "a, b = predict_dataloader(test_dataloader)\n",
    "\n",
    "a = np.hstack(a)\n",
    "b = np.hstack(b)\n",
    "\n",
    "for x in zip(a,b):\n",
    "    print(x[0], x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0e20fd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16929/16929 [04:50<00:00, 58.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.664\n"
     ]
    }
   ],
   "source": [
    "def acc():\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0,0\n",
    "    total_preds = []\n",
    "    total_acc_test = 0\n",
    "\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), total = len(train_dataloader)):\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "            #labels= labels.unsqueeze(1)\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "            \n",
    "            test_labels = labels.to(device)\n",
    "            acc = (preds.argmax(dim=1) == test_labels).sum().item()\n",
    "            total_acc_test += acc            \n",
    "            \n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "            \n",
    "\n",
    "\n",
    "    avg_loss = total_loss / len(test_dataloader)\n",
    "    total_preds = np.concatenate(total_preds, axis = 0)\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(total_preds): .3f}')\n",
    "    return avg_loss, total_preds\n",
    "\n",
    "a,b = acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4139aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.88      0.78        16\n",
      "           1       0.90      0.76      0.82       140\n",
      "           2       0.91      0.64      0.75       242\n",
      "           3       0.63      0.76      0.69        75\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.87      0.98      0.92       167\n",
      "           6       0.83      0.49      0.61       212\n",
      "           7       0.71      0.76      0.73       221\n",
      "           8       0.56      0.68      0.62        47\n",
      "           9       0.58      0.48      0.53        31\n",
      "          10       0.46      0.85      0.60        20\n",
      "          11       0.50      0.66      0.57        32\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.46      0.72      0.56        43\n",
      "          15       0.79      0.40      0.53       176\n",
      "          16       0.34      0.62      0.44        48\n",
      "          17       0.80      0.54      0.65       133\n",
      "          18       0.57      0.57      0.57       127\n",
      "          19       0.75      0.65      0.69       190\n",
      "          20       0.31      0.40      0.35        86\n",
      "          21       0.83      0.84      0.84       286\n",
      "          22       0.55      0.62      0.58        42\n",
      "          23       0.50      1.00      0.67         8\n",
      "          24       0.18      0.33      0.23        24\n",
      "          25       0.07      1.00      0.13         1\n",
      "          26       0.78      0.63      0.70       368\n",
      "          27       0.47      0.49      0.48       126\n",
      "          28       0.69      0.44      0.54       336\n",
      "          29       0.79      0.77      0.78       226\n",
      "          30       0.04      1.00      0.08         1\n",
      "          31       0.17      0.46      0.24        52\n",
      "          32       0.63      0.64      0.63       249\n",
      "          33       0.23      0.54      0.32        89\n",
      "          34       0.03      0.20      0.05         5\n",
      "          35       0.00      0.00      0.00         0\n",
      "          36       0.00      0.00      0.00         0\n",
      "          37       0.10      0.39      0.16        46\n",
      "          38       0.70      0.41      0.52      1913\n",
      "          39       0.78      0.87      0.83       929\n",
      "          40       0.75      0.60      0.67        10\n",
      "          41       0.75      0.63      0.68       256\n",
      "          42       0.62      0.64      0.63        25\n",
      "          43       0.88      0.90      0.89       702\n",
      "          44       0.00      0.00      0.00         0\n",
      "          45       0.00      0.00      0.00         3\n",
      "          46       0.57      0.89      0.70         9\n",
      "          47       0.68      0.51      0.58       386\n",
      "          48       0.45      0.68      0.55        44\n",
      "          49       0.00      0.00      0.00         0\n",
      "          50       0.50      0.30      0.38        23\n",
      "          51       0.83      0.63      0.72        62\n",
      "          52       0.22      0.67      0.33         3\n",
      "          53       0.13      0.29      0.18        17\n",
      "          54       0.20      0.48      0.28        21\n",
      "          55       0.37      0.35      0.36        54\n",
      "          56       0.54      0.70      0.61        27\n",
      "          57       0.00      0.00      0.00         1\n",
      "          58       0.00      0.00      0.00         0\n",
      "          59       0.10      0.33      0.15         6\n",
      "          60       0.80      0.87      0.83      1071\n",
      "          61       0.85      0.77      0.81      1084\n",
      "          62       0.29      0.46      0.36        91\n",
      "          63       0.98      0.82      0.89       509\n",
      "          64       0.14      0.44      0.21        25\n",
      "          65       0.40      1.00      0.57         2\n",
      "          66       0.20      1.00      0.33         2\n",
      "          67       0.44      0.43      0.43       152\n",
      "          68       0.52      0.96      0.68        70\n",
      "          69       0.67      0.65      0.66       242\n",
      "          70       0.43      0.89      0.58        18\n",
      "          71       0.82      0.65      0.72       275\n",
      "          72       0.64      0.79      0.71      1281\n",
      "          73       0.70      0.60      0.65       120\n",
      "          74       0.00      0.00      0.00         0\n",
      "          75       0.76      0.66      0.71       178\n",
      "          76       0.00      0.00      0.00         0\n",
      "          77       0.00      0.00      0.00         0\n",
      "          79       0.21      0.26      0.23        27\n",
      "          80       0.52      0.63      0.57       282\n",
      "          81       0.48      0.49      0.49       315\n",
      "          82       0.56      0.67      0.61      2330\n",
      "          83       0.75      0.78      0.76      2788\n",
      "          84       0.41      1.00      0.58        13\n",
      "          85       0.81      0.59      0.68      1911\n",
      "          86       0.50      1.00      0.67         6\n",
      "          87       0.25      0.67      0.36         3\n",
      "          88       0.48      0.61      0.54       499\n",
      "          89       0.72      0.92      0.81        62\n",
      "          90       0.62      0.67      0.64        12\n",
      "          91       0.00      0.00      0.00         0\n",
      "          92       0.67      0.63      0.65       422\n",
      "          93       0.80      0.61      0.69       202\n",
      "          94       0.19      0.23      0.21       116\n",
      "          95       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.67     22465\n",
      "   macro avg       0.46      0.55      0.47     22465\n",
      "weighted avg       0.71      0.67      0.68     22465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugene/miniconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c1a201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
